{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00faf4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json as js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36563027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ALL CONSTANTS\n",
    "\n",
    "#main URL\n",
    "main_url = 'https://simple.wikipedia.org'\n",
    "\n",
    "#initial URL\n",
    "init_url = main_url + '/wiki/Climate_change'\n",
    "\n",
    "#processing keys\n",
    "key_data = 'data'\n",
    "key_code = 'code'\n",
    "key_a = 'a'\n",
    "key_div = 'div'\n",
    "key_script = 'script'\n",
    "key_footer = 'footer'\n",
    "key_internal = 'internal'\n",
    "key_external = 'external'\n",
    "key_local = 'local'\n",
    "key_extra = 'extra'\n",
    "key_page_cnt = 'Pagecount'\n",
    "key_int_cnt = 'INTcount'\n",
    "key_ext_cnt = 'EXTcount'\n",
    "key_url_cnt = 'URLfragments'\n",
    "key_tmstp = 'timestamp'\n",
    "\n",
    "#HTML attributes\n",
    "attr_cnt = {'class':'mw-body', 'id':'content', 'role':'main'}\n",
    "attr_ftr = {'class':'mw-footer', 'role':'contentinfo'}\n",
    "attr_nav = {'id':'mw-panel'}\n",
    "attr_nhead = {'id':'mw-head'}\n",
    "attr_scr = {'type':'application/ld+json'}\n",
    "\n",
    "#runtime error message\n",
    "run_err_msg = 'Runtime Processing Error'\n",
    "\n",
    "#response codes\n",
    "code_success = 200\n",
    "code_err = 999\n",
    "code_missing = 404\n",
    "\n",
    "#HTML section names\n",
    "main_body = 'Main content'\n",
    "side_nav = 'Side navigation'\n",
    "top_nav = 'Top navigation'\n",
    "footer = 'footer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39fcb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functin for printing run time error\n",
    "def printRuntimeError(err):\n",
    "    \n",
    "    #print error received in arguments\n",
    "    print(f'Unexpected {err=}, {type(err)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16fa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for getting HTML content of provided URL \n",
    "def getHTMLPage(web_url):\n",
    "    \n",
    "    #setting intitial response\n",
    "    response = {key_code:code_success, key_data: None}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #retreiving HTML content of given URL using requests\n",
    "        result = rq.get(web_url)\n",
    "        \n",
    "        #verifying response of HTTP request\n",
    "        if result.status_code == code_success:\n",
    "            \n",
    "            #converting HTML text in BeautifulSoup object\n",
    "            html_content = bs(result.text, 'html.parser')\n",
    "            \n",
    "            #setting response\n",
    "            response = {key_code:code_success, key_data: html_content}\n",
    "        else:\n",
    "            \n",
    "            #setting fail response\n",
    "            response = {key_code:result.status_code, key_data: 'Unable to process given URL'}\n",
    "            \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #runtime error response\n",
    "        response = {key_code:code_err, key_data: run_err_msg}\n",
    "    \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9a1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for retreiving specified section from given HTML content\n",
    "def getHTMLSection(bs_object, tag_name, tag_attrs):\n",
    "    \n",
    "    #setting intitial response\n",
    "    response = {key_code:code_success, key_data: None}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #finding HTML section using tag and attributes\n",
    "        section = bs_object.find(tag_name, attrs=tag_attrs)\n",
    "        \n",
    "        #setting response\n",
    "        response = {key_code: code_success, key_data: section}\n",
    "        \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "    \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a3ddc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for retreiving anchor tags from given HTML content \n",
    "def getAnchorTags(bs_object, tag_name):\n",
    "    \n",
    "    #setting intitial response\n",
    "    response = {key_code:code_success, key_data: None}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #finding all anchor tags from HTML content \n",
    "        tags = bs_object.find_all(tag_name, href=True)\n",
    "        \n",
    "        #setting response\n",
    "        response = {key_code: code_success, key_data: tags}\n",
    "        \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "    \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40c3bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for extracting timestamp from given content\n",
    "def getTimeStamp(scr_data):\n",
    "    \n",
    "    #setting intitial response\n",
    "    response = {key_code:code_missing, key_data: None}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #setting constant key of last page modification in variable\n",
    "        date_key = 'dateModified'\n",
    "        \n",
    "        #verifying whether content is available\n",
    "        if scr_data != None:\n",
    "            \n",
    "            #converting plain text in JSON using json librabry\n",
    "            json_data = js.loads(scr_data.string)\n",
    "        \n",
    "            #verifying whether last modification is available\n",
    "            if date_key in json_data:\n",
    "                \n",
    "                #setting response\n",
    "                response = {key_code: code_success, key_data: json_data[date_key]}\n",
    "                \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "    \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "babb4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for categorising URL's into specified category\n",
    "def identifyLinks(data, cur_url):\n",
    "    \n",
    "    #setting intitial response\n",
    "    response = {key_code: code_success, key_data: {key_local:[], key_internal:[], key_external:[], key_extra:[]}}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #iterating over all the given URLS\n",
    "        for a in data:\n",
    "            \n",
    "            #extracting HREF from anchor tags\n",
    "            href = a['href']\n",
    "            \n",
    "            #verifying whether iterating URL similar to crawled page\n",
    "            if cur_url == href:\n",
    "                \n",
    "                #adding crawled page URL to EXTRA for avoiding repeatation crawling\n",
    "                response[key_data][key_extra].append(f'{main_url}{href}')\n",
    "            \n",
    "            #verifying whether iterating URL is internal\n",
    "            elif href.startswith('/wiki'):\n",
    "                \n",
    "                #adding internal page URL to INTERNAL with appending main url\n",
    "                response[key_data][key_internal].append(f'{main_url}{href}')\n",
    "                \n",
    "            #verifying whether iterating URL is local\n",
    "            elif href.startswith('#'):\n",
    "                \n",
    "                #adding local page URL to LOCAL\n",
    "                response[key_data][key_local].append(f'{href}')\n",
    "                \n",
    "            #verifying whether iterating URL is external\n",
    "            elif href.startswith('http') or href.startswith('https') or href.startswith('//'):\n",
    "                \n",
    "                #adding external page URL to EXTERNAL\n",
    "                response[key_data][key_external].append(f'{href}')\n",
    "                \n",
    "            #verifying whether iterating URL is internal but leads to page action\n",
    "            elif href.startswith('/'):\n",
    "                \n",
    "                #adding internal page URL to INTERNAL as iterating url links to same domain\n",
    "                response[key_data][key_extra].append(f'{main_url}{href}')\n",
    "                \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "        \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d4f3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for getting section data from given data dictionary\n",
    "def getAnchorFromSection(sec_dict, sec_name, url):\n",
    "    #setting initial response\n",
    "    response = None\n",
    "    \n",
    "    #verifying whether data dictionary has SUCCESS response\n",
    "    if sec_dict[key_code] == code_success:\n",
    "        \n",
    "        #getting anchor tags and setting response\n",
    "        response = getAnchorTags(sec_dict[key_data], key_a)\n",
    "    else:\n",
    "        \n",
    "        #printing message for failed response\n",
    "        print(f'{sec_name} section extraction failed for URL: {url}')\n",
    "        \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "365a7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for getting links data from given data dictionary\n",
    "def getIdentifiedLinks(link_dict, cur_url, sec_name, url):\n",
    "    \n",
    "    #setting initial response\n",
    "    response = None\n",
    "    \n",
    "    #verifying whether data dictionary has SUCCESS response\n",
    "    if link_dict[key_code] == code_success:\n",
    "        \n",
    "        #identifying links and setting response\n",
    "        response = identifyLinks(link_dict[key_data], cur_url)\n",
    "    else:\n",
    "        \n",
    "        #printing message for failed response\n",
    "        print(f'{sec_name} link extraction failed for URL: {url}')\n",
    "        \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a45def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for generating crawled page report\n",
    "def generateReport(report, url_list, lnk_dict, sec_name, url):\n",
    "    \n",
    "    #verifying whether data dictionary is available and response is success\n",
    "    if lnk_dict != None and lnk_dict[key_code] == code_success:\n",
    "        \n",
    "        #selecting data from data dictionary\n",
    "        lnk_dict = lnk_dict[key_data]\n",
    "        \n",
    "        #appending list of internal URLS to given URL list for processing \n",
    "        url_list.extend(lnk_dict[key_internal])\n",
    "        \n",
    "        #calculating INTERNAL links\n",
    "        report[key_int_cnt] += len(lnk_dict[key_internal]) + len(lnk_dict[key_extra])\n",
    "        \n",
    "        #calculating EXTERNAL links\n",
    "        report[key_ext_cnt] += len(lnk_dict[key_external])\n",
    "        \n",
    "        #calculating LOCAL links\n",
    "        report[key_url_cnt] += len(lnk_dict[key_local])\n",
    "    else:\n",
    "        \n",
    "        #printing message for failed response\n",
    "        print(f'{sec_name} link identification failed for URL: {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5447ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for web page crawling process\n",
    "def processCrawling(start_url):\n",
    "    \n",
    "    #setting initial response\n",
    "    response = {key_code:code_success, key_data: []}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        #setting page count\n",
    "        total_crawl = 1\n",
    "        \n",
    "        #setting list index\n",
    "        li_index = -1\n",
    "        \n",
    "        #defining URL list\n",
    "        url_list = [start_url]\n",
    "        \n",
    "        #defining visited URL list\n",
    "        visited_urls = []\n",
    "        \n",
    "        #defining loop for crawling only 200 pages\n",
    "        while total_crawl <= 200:\n",
    "            \n",
    "            #incrementing list index for URL list index\n",
    "            li_index += 1\n",
    "            \n",
    "            #accessing URL from list\n",
    "            url = url_list[li_index]\n",
    "            \n",
    "            #verifying whether URL is already processed or not, if it is then continue\n",
    "            if url in visited_urls:\n",
    "                continue\n",
    "            else:\n",
    "                #adding current URL to already visited URLS\n",
    "                visited_urls.append(url)\n",
    "            \n",
    "            #printing message for current processing URL\n",
    "            print(f'Processing Index: {total_crawl}, URL is: {url}')\n",
    "            \n",
    "            #retreiving HTML page of URL\n",
    "            html_page = getHTMLPage(url)\n",
    "            \n",
    "            #verifying whether retreiving HTML content was success or not\n",
    "            if html_page[key_code] == code_success:\n",
    "                \n",
    "                #accessing HTML data from HTML data dictionary\n",
    "                data = html_page[key_data]\n",
    "                    \n",
    "                #accessing MAIN BODY HTML section from HTML content\n",
    "                cnt_sec = getHTMLSection(data, key_div, attr_cnt)\n",
    "                \n",
    "                #accessing SIDE NAVIGATION HTML section from HTML content\n",
    "                nav_sec = getHTMLSection(data, key_div, attr_nav)\n",
    "                \n",
    "                #accessing FOOTER HTML section from HTML content\n",
    "                ftr_sec = getHTMLSection(data, key_footer, attr_ftr)\n",
    "                \n",
    "                #accessing SCRIPT HTML section from HTML content for timestamp\n",
    "                scr_sec = getHTMLSection(data, key_script, attr_scr)\n",
    "                \n",
    "                #accessing TOP NAVIGATION HTML section from HTML content\n",
    "                navh_sec = getHTMLSection(data, key_div, attr_nhead)\n",
    "                \n",
    "                #defining variable for storing timestamp\n",
    "                last_modified = None\n",
    "                \n",
    "                #verifying whether SCRIPT section extraction was successfull\n",
    "                if scr_sec[key_code] == code_success:\n",
    "                    \n",
    "                    #extracting TIMESTAMP from script text \n",
    "                    ts_result = getTimeStamp(scr_sec[key_data])\n",
    "                    \n",
    "                    #checking if TIMESTAMP is available and storing in variable\n",
    "                    last_modified = ts_result[key_data] if ts_result[key_code] == code_success else None \n",
    "                        \n",
    "                #accessing MAIN BODY links from HTML section\n",
    "                cnt_link = getAnchorFromSection(cnt_sec, main_body, url)\n",
    "                \n",
    "                #accessing FOOTER links from HTML section\n",
    "                ftr_link = getAnchorFromSection(ftr_sec, footer, url)\n",
    "                \n",
    "                #accessing SIDE NAVIGATION links from HTML section\n",
    "                nav_link = getAnchorFromSection(nav_sec, side_nav, url)\n",
    "                \n",
    "                #accessing TOP NAVIGATION links from HTML section\n",
    "                navh_link = getAnchorFromSection(navh_sec, top_nav, url)\n",
    "                    \n",
    "                #separating current pafe URI from FULL URL\n",
    "                cur_url = url.replace(main_url, '')\n",
    "\n",
    "                #retreiving MAIN CONTENT's identified links from all links\n",
    "                cnt_lnk = getIdentifiedLinks(cnt_link, cur_url, main_body, url)\n",
    "                \n",
    "                #retreiving FOOTER's identified links from all links\n",
    "                ftr_lnk = getIdentifiedLinks(ftr_link, cur_url, footer, url)\n",
    "                \n",
    "                #retreiving SIDE NAVIGATION's identified links from all links\n",
    "                nav_lnk = getIdentifiedLinks(nav_link, cur_url, side_nav, url)\n",
    "                \n",
    "                #retreiving TOP NAVIGATION's identified links from all links\n",
    "                navh_lnk = getIdentifiedLinks(navh_link, cur_url, top_nav, url)\n",
    "                    \n",
    "                #defining page report data dictionary\n",
    "                page_report = {key_page_cnt: total_crawl, key_int_cnt: 0, \n",
    "                                key_ext_cnt: 0, key_url_cnt:0, key_tmstp: last_modified}\n",
    "                    \n",
    "                #calculating report for MAIN BODY content and appending INTERNAL links for processing\n",
    "                generateReport(page_report, url_list, cnt_lnk, main_body, url)\n",
    "                \n",
    "                #calculating report for FOOTER content and appending INTERNAL links for processing\n",
    "                generateReport(page_report, url_list, ftr_lnk, footer, url)\n",
    "                \n",
    "                #calculating report for SIDE NAVIGATION content and appending INTERNAL links for processing\n",
    "                generateReport(page_report, url_list, nav_lnk, side_nav, url)\n",
    "                \n",
    "                #calculating report for TOP NAVIGATION content and appending INTERNAL links for processing\n",
    "                generateReport(page_report, url_list, navh_lnk, top_nav, url)\n",
    "                \n",
    "                #appending page report to response\n",
    "                response[key_data].append(page_report)\n",
    "                \n",
    "                #incrementing page counts\n",
    "                total_crawl += 1\n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "        \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6c2cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for converting dictionary into dataframe and writing csv file\n",
    "def dictToDataframe(data):\n",
    "    \n",
    "    #setting initial response\n",
    "    response = {key_code: code_success, key_data: 'Process completed successfully...'}\n",
    "    \n",
    "    #wrapping code in try except block for runtime error handling\n",
    "    try:\n",
    "        \n",
    "        #generating dataframe from given data dictionary\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        #writing dataframe to csv\n",
    "        df.to_csv('data.csv', index = False)\n",
    "        \n",
    "    except BaseException as err:\n",
    "        #print rutime error\n",
    "        printRuntimeError(err)\n",
    "        \n",
    "        #setting runtime error message\n",
    "        response = {key_code: code_err, key_data: run_err_msg}\n",
    "        \n",
    "    #returning response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1df6ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Index: 1, URL is: https://simple.wikipedia.org/wiki/Climate_change\n",
      "Processing Index: 2, URL is: https://simple.wikipedia.org/wiki/Global_warming\n",
      "Processing Index: 3, URL is: https://simple.wikipedia.org/wiki/Climate\n",
      "Processing Index: 4, URL is: https://simple.wikipedia.org/wiki/Earth\n",
      "Processing Index: 5, URL is: https://simple.wikipedia.org/wiki/Ice_age\n",
      "Processing Index: 6, URL is: https://simple.wikipedia.org/wiki/Earth%27s_orbit\n",
      "Processing Index: 7, URL is: https://simple.wikipedia.org/wiki/Sun\n",
      "Processing Index: 8, URL is: https://simple.wikipedia.org/wiki/Greenhouse_gas\n",
      "Processing Index: 9, URL is: https://simple.wikipedia.org/wiki/Joseph_Fourier\n",
      "Processing Index: 10, URL is: https://simple.wikipedia.org/wiki/John_Tyndall\n",
      "Processing Index: 11, URL is: https://simple.wikipedia.org/wiki/Svante_Arrhenius\n",
      "Processing Index: 12, URL is: https://simple.wikipedia.org/wiki/Nils_Gustaf_Ekholm\n",
      "Processing Index: 13, URL is: https://simple.wikipedia.org/wiki/Sunspot\n",
      "Processing Index: 14, URL is: https://simple.wikipedia.org/wiki/Stratosphere\n",
      "Processing Index: 15, URL is: https://simple.wikipedia.org/wiki/Ecology\n",
      "Processing Index: 16, URL is: https://simple.wikipedia.org/wiki/ISSN_(identifier)\n",
      "Processing Index: 17, URL is: https://simple.wikipedia.org/wiki/Bibcode_(identifier)\n",
      "Processing Index: 18, URL is: https://simple.wikipedia.org/wiki/Doi_(identifier)\n",
      "Processing Index: 19, URL is: https://simple.wikipedia.org/wiki/Special:Categories\n",
      "Processing Index: 20, URL is: https://simple.wikipedia.org/wiki/Category:Climate_change\n",
      "Processing Index: 21, URL is: https://simple.wikipedia.org/wiki/Wikipedia:About\n",
      "Processing Index: 22, URL is: https://simple.wikipedia.org/wiki/Wikipedia:General_disclaimer\n",
      "Processing Index: 23, URL is: https://simple.wikipedia.org/wiki/Main_Page\n",
      "Processing Index: 24, URL is: https://simple.wikipedia.org/wiki/Wikipedia:Simple_start\n",
      "Processing Index: 25, URL is: https://simple.wikipedia.org/wiki/Wikipedia:Simple_talk\n",
      "Processing Index: 26, URL is: https://simple.wikipedia.org/wiki/Special:RecentChanges\n",
      "Processing Index: 27, URL is: https://simple.wikipedia.org/wiki/Special:Random\n",
      "Processing Index: 28, URL is: https://simple.wikipedia.org/wiki/Help:Contents\n",
      "Processing Index: 29, URL is: https://simple.wikipedia.org/wiki/Special:WhatLinksHere/Climate_change\n",
      "Processing Index: 30, URL is: https://simple.wikipedia.org/wiki/Special:RecentChangesLinked/Climate_change\n",
      "Processing Index: 31, URL is: https://simple.wikipedia.org/wiki/Special:SpecialPages\n",
      "Processing Index: 32, URL is: https://simple.wikipedia.org/wiki/Special:MyTalk\n",
      "Processing Index: 33, URL is: https://simple.wikipedia.org/wiki/Special:MyContributions\n",
      "Processing Index: 34, URL is: https://simple.wikipedia.org/wiki/Talk:Climate_change\n",
      "Processing Index: 35, URL is: https://simple.wikipedia.org/wiki/Climate_variability_and_change\n",
      "Processing Index: 36, URL is: https://simple.wikipedia.org/wiki/File:20200324_Global_average_temperature_-_NASA-GISS_HadCrut_NOAA_Japan_BerkeleyE.svg\n",
      "Processing Index: 37, URL is: https://simple.wikipedia.org/wiki/Industrial_Revolution\n",
      "Processing Index: 38, URL is: https://simple.wikipedia.org/wiki/File:Change_in_Average_Temperature.svg\n",
      "Processing Index: 39, URL is: https://simple.wikipedia.org/wiki/Weather\n",
      "Processing Index: 40, URL is: https://simple.wikipedia.org/wiki/Temperature\n",
      "Processing Index: 41, URL is: https://simple.wikipedia.org/wiki/Atmosphere\n",
      "Processing Index: 42, URL is: https://simple.wikipedia.org/wiki/Scientist\n",
      "Processing Index: 43, URL is: https://simple.wikipedia.org/wiki/Greenland\n",
      "Processing Index: 44, URL is: https://simple.wikipedia.org/wiki/Gasoline\n",
      "Processing Index: 45, URL is: https://simple.wikipedia.org/wiki/Natural_gas\n",
      "Processing Index: 46, URL is: https://simple.wikipedia.org/wiki/Carbon_dioxide\n",
      "Processing Index: 47, URL is: https://simple.wikipedia.org/wiki/Combustion\n",
      "Processing Index: 48, URL is: https://simple.wikipedia.org/wiki/Fossil_fuels\n",
      "Processing Index: 49, URL is: https://simple.wikipedia.org/wiki/Coal\n",
      "Processing Index: 50, URL is: https://simple.wikipedia.org/wiki/Oil\n",
      "Processing Index: 51, URL is: https://simple.wikipedia.org/wiki/Carbon\n",
      "Processing Index: 52, URL is: https://simple.wikipedia.org/wiki/Atom\n",
      "Processing Index: 53, URL is: https://simple.wikipedia.org/wiki/Oxygen\n",
      "Processing Index: 54, URL is: https://simple.wikipedia.org/wiki/Deforestation\n",
      "Processing Index: 55, URL is: https://simple.wikipedia.org/wiki/Plants\n",
      "Processing Index: 56, URL is: https://simple.wikipedia.org/wiki/Sea_level\n",
      "Processing Index: 57, URL is: https://simple.wikipedia.org/wiki/Thermal_expansion\n",
      "Processing Index: 58, URL is: https://simple.wikipedia.org/wiki/Glacier\n",
      "Processing Index: 59, URL is: https://simple.wikipedia.org/wiki/Ice_cap\n",
      "Processing Index: 60, URL is: https://simple.wikipedia.org/wiki/Coast\n",
      "Processing Index: 61, URL is: https://simple.wikipedia.org/wiki/Flood\n",
      "Processing Index: 62, URL is: https://simple.wikipedia.org/wiki/Precipitation\n",
      "Processing Index: 63, URL is: https://simple.wikipedia.org/wiki/Desert\n",
      "Processing Index: 64, URL is: https://simple.wikipedia.org/wiki/Storm\n",
      "Processing Index: 65, URL is: https://simple.wikipedia.org/wiki/Farming\n",
      "Processing Index: 66, URL is: https://simple.wikipedia.org/wiki/Food\n",
      "Processing Index: 67, URL is: https://simple.wikipedia.org/wiki/The_Washington_Post\n",
      "Processing Index: 68, URL is: https://simple.wikipedia.org/wiki/Government\n",
      "Processing Index: 69, URL is: https://simple.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change\n",
      "Processing Index: 70, URL is: https://simple.wikipedia.org/wiki/Company\n",
      "Processing Index: 71, URL is: https://simple.wikipedia.org/wiki/Kyoto_Protocol\n",
      "Processing Index: 72, URL is: https://simple.wikipedia.org/wiki/Paris_Agreement\n",
      "Processing Index: 73, URL is: https://simple.wikipedia.org/wiki/Pollution\n",
      "Processing Index: 74, URL is: https://simple.wikipedia.org/wiki/Methane\n",
      "Processing Index: 75, URL is: https://simple.wikipedia.org/wiki/Temperature_record_of_the_past_1000_years\n",
      "Processing Index: 76, URL is: https://simple.wikipedia.org/wiki/File:2000%2B_year_global_temperature_including_Medieval_Warm_Period_and_Little_Ice_Age_-_Ed_Hawkins.svg\n",
      "Processing Index: 77, URL is: https://simple.wikipedia.org/wiki/Medieval_Warm_Period\n",
      "Processing Index: 78, URL is: https://simple.wikipedia.org/wiki/Little_Ice_Age\n",
      "Processing Index: 79, URL is: https://simple.wikipedia.org/wiki/History_of_the_Earth\n",
      "Processing Index: 80, URL is: https://simple.wikipedia.org/wiki/Average\n",
      "Processing Index: 81, URL is: https://simple.wikipedia.org/wiki/Industry\n",
      "Processing Index: 82, URL is: https://simple.wikipedia.org/wiki/Satellite_(artificial)\n",
      "Processing Index: 83, URL is: https://simple.wikipedia.org/wiki/Climatology\n",
      "Processing Index: 84, URL is: https://simple.wikipedia.org/wiki/Proxy\n",
      "Processing Index: 85, URL is: https://simple.wikipedia.org/wiki/Thermometer\n",
      "Processing Index: 86, URL is: https://simple.wikipedia.org/wiki/Growth_ring\n",
      "Processing Index: 87, URL is: https://simple.wikipedia.org/wiki/Rain\n",
      "Processing Index: 88, URL is: https://simple.wikipedia.org/wiki/Ice_core\n",
      "Processing Index: 89, URL is: https://simple.wikipedia.org/wiki/Greenhouse_effect\n",
      "Processing Index: 90, URL is: https://simple.wikipedia.org/wiki/File:Global_Warming_Observed_CO2_Emissions_from_fossil_fuel_burning_vs_IPCC_scenarios.svg\n",
      "Processing Index: 91, URL is: https://simple.wikipedia.org/wiki/Car\n",
      "Processing Index: 92, URL is: https://simple.wikipedia.org/wiki/Chimney\n",
      "Processing Index: 93, URL is: https://simple.wikipedia.org/wiki/Fossil_fuel\n",
      "Processing Index: 94, URL is: https://simple.wikipedia.org/wiki/Dust\n",
      "Processing Index: 95, URL is: https://simple.wikipedia.org/wiki/Dirt\n",
      "Processing Index: 96, URL is: https://simple.wikipedia.org/wiki/Volcano\n",
      "Processing Index: 97, URL is: https://simple.wikipedia.org/wiki/Erosion\n",
      "Processing Index: 98, URL is: https://simple.wikipedia.org/wiki/Meteor\n",
      "Processing Index: 99, URL is: https://simple.wikipedia.org/wiki/Aerosol\n",
      "Processing Index: 100, URL is: https://simple.wikipedia.org/wiki/Energy_conservation\n",
      "Processing Index: 101, URL is: https://simple.wikipedia.org/wiki/Solar_panel\n",
      "Processing Index: 102, URL is: https://simple.wikipedia.org/wiki/Nuclear_power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Index: 103, URL is: https://simple.wikipedia.org/wiki/Wind_power\n",
      "Processing Index: 104, URL is: https://simple.wikipedia.org/wiki/Nanotechnology\n",
      "Processing Index: 105, URL is: https://simple.wikipedia.org/wiki/Ethanol\n",
      "Processing Index: 106, URL is: https://simple.wikipedia.org/wiki/File:Joseph_Fourier.jpg\n",
      "Processing Index: 107, URL is: https://simple.wikipedia.org/wiki/File:Arrhenius2.jpg\n",
      "Processing Index: 108, URL is: https://simple.wikipedia.org/wiki/Celsius\n",
      "Processing Index: 109, URL is: https://simple.wikipedia.org/wiki/Wikipedia:Citing_sources\n",
      "Processing Index: 110, URL is: https://simple.wikipedia.org/wiki/Antarctica\n",
      "Processing Index: 111, URL is: https://simple.wikipedia.org/wiki/Science_(journal)\n",
      "Processing Index: 112, URL is: https://simple.wikipedia.org/wiki/Bangladesh\n",
      "Processing Index: 113, URL is: https://simple.wikipedia.org/wiki/Florida\n",
      "Processing Index: 114, URL is: https://simple.wikipedia.org/wiki/Netherland\n",
      "Processing Index: 115, URL is: https://simple.wikipedia.org/wiki/File:6m_Sea_Level_Rise.jpg\n",
      "Processing Index: 116, URL is: https://simple.wikipedia.org/wiki/Port\n",
      "Processing Index: 117, URL is: https://simple.wikipedia.org/wiki/Storm_surge\n",
      "Processing Index: 118, URL is: https://simple.wikipedia.org/wiki/Wikipedia:Reliable_sources\n",
      "Processing Index: 119, URL is: https://simple.wikipedia.org/wiki/London\n",
      "Processing Index: 120, URL is: https://simple.wikipedia.org/wiki/New_York_City\n",
      "Processing Index: 121, URL is: https://simple.wikipedia.org/wiki/Norfolk,_Virginia\n",
      "Processing Index: 122, URL is: https://simple.wikipedia.org/wiki/Hampton_Roads\n",
      "Processing Index: 123, URL is: https://simple.wikipedia.org/wiki/United_States\n",
      "Processing Index: 124, URL is: https://simple.wikipedia.org/wiki/Southampton\n",
      "Processing Index: 125, URL is: https://simple.wikipedia.org/wiki/Crisfield,_Maryland\n",
      "Processing Index: 126, URL is: https://simple.wikipedia.org/wiki/Charleston,_South_Carolina\n",
      "Processing Index: 127, URL is: https://simple.wikipedia.org/wiki/Miami\n",
      "Processing Index: 128, URL is: https://simple.wikipedia.org/wiki/Saint_Petersburg\n",
      "Processing Index: 129, URL is: https://simple.wikipedia.org/wiki/Sydney\n",
      "Processing Index: 130, URL is: https://simple.wikipedia.org/wiki/Australia\n",
      "Processing Index: 131, URL is: https://simple.wikipedia.org/wiki/Jakarta\n",
      "Processing Index: 132, URL is: https://simple.wikipedia.org/wiki/Badin\n",
      "Processing Index: 133, URL is: https://simple.wikipedia.org/wiki/Sindh\n",
      "Processing Index: 134, URL is: https://simple.wikipedia.org/wiki/Pakistan\n",
      "Processing Index: 135, URL is: https://simple.wikipedia.org/wiki/Mal%C3%A9\n",
      "Processing Index: 136, URL is: https://simple.wikipedia.org/wiki/Maldives\n",
      "Processing Index: 137, URL is: https://simple.wikipedia.org/wiki/Mumbai\n",
      "Processing Index: 138, URL is: https://simple.wikipedia.org/wiki/Buenos_Aires\n",
      "Processing Index: 139, URL is: https://simple.wikipedia.org/wiki/Los_Angeles\n",
      "Processing Index: 140, URL is: https://simple.wikipedia.org/wiki/Rio_de_Janeiro\n",
      "Processing Index: 141, URL is: https://simple.wikipedia.org/wiki/National_Geographic\n",
      "Processing Index: 142, URL is: https://simple.wikipedia.org/wiki/BBC\n",
      "Processing Index: 143, URL is: https://simple.wikipedia.org/wiki/James_Hansen\n",
      "Processing Index: 144, URL is: https://simple.wikipedia.org/wiki/Stern_Review\n",
      "Processing Index: 145, URL is: https://simple.wikipedia.org/wiki/IPCC\n",
      "Processing Index: 146, URL is: https://simple.wikipedia.org/wiki/New_York_Times\n",
      "Processing Index: 147, URL is: https://simple.wikipedia.org/wiki/Scientific_American\n",
      "Processing Index: 148, URL is: https://simple.wikipedia.org/wiki/Popular_Mechanics\n",
      "Processing Index: 149, URL is: https://simple.wikipedia.org/wiki/The_Guardian\n",
      "Processing Index: 150, URL is: https://simple.wikipedia.org/wiki/Category:CS1_maint:_extra_text:_authors_list\n",
      "Processing Index: 151, URL is: https://simple.wikipedia.org/wiki/YouTube\n",
      "Processing Index: 152, URL is: https://simple.wikipedia.org/wiki/Wayback_Machine\n",
      "Processing Index: 153, URL is: https://simple.wikipedia.org/wiki/Royal_Geographical_Society\n",
      "Processing Index: 154, URL is: https://simple.wikipedia.org/wiki/World_Bank\n",
      "Processing Index: 155, URL is: https://simple.wikipedia.org/wiki/Washington_Post\n",
      "Processing Index: 156, URL is: https://simple.wikipedia.org/wiki/Wikipedia:Sister_projects\n",
      "Processing Index: 157, URL is: https://simple.wikipedia.org/wiki/Category:Energy\n",
      "Processing Index: 158, URL is: https://simple.wikipedia.org/wiki/Category:Air_pollution\n",
      "Processing Index: 159, URL is: https://simple.wikipedia.org/wiki/Category:Webarchive_template_wayback_links\n",
      "Processing Index: 160, URL is: https://simple.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements\n",
      "Processing Index: 161, URL is: https://simple.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_June_2020\n",
      "Processing Index: 162, URL is: https://simple.wikipedia.org/wiki/Special:WhatLinksHere/Global_warming\n",
      "Processing Index: 163, URL is: https://simple.wikipedia.org/wiki/Special:RecentChangesLinked/Global_warming\n",
      "Processing Index: 164, URL is: https://simple.wikipedia.org/wiki/Talk:Global_warming\n",
      "Processing Index: 165, URL is: https://simple.wikipedia.org/wiki/File:ClimateMap_World.png\n",
      "Processing Index: 166, URL is: https://simple.wikipedia.org/wiki/File:Koppen-Geiger_Map_World_present.svg\n",
      "Processing Index: 167, URL is: https://simple.wikipedia.org/wiki/Tropical_climate\n",
      "Processing Index: 168, URL is: https://simple.wikipedia.org/wiki/Desert_climate\n",
      "Processing Index: 169, URL is: https://simple.wikipedia.org/wiki/Temperate_zone\n",
      "Processing Index: 170, URL is: https://simple.wikipedia.org/wiki/Polar_climate\n",
      "Processing Index: 171, URL is: https://simple.wikipedia.org/wiki/Mediterranean_climate\n",
      "Processing Index: 172, URL is: https://simple.wikipedia.org/wiki/Temperate\n",
      "Processing Index: 173, URL is: https://simple.wikipedia.org/wiki/Season\n",
      "Processing Index: 174, URL is: https://simple.wikipedia.org/wiki/Europe\n",
      "Processing Index: 175, URL is: https://simple.wikipedia.org/wiki/Saudi_Arabia\n",
      "Processing Index: 176, URL is: https://simple.wikipedia.org/wiki/Amazon_Rainforest\n",
      "Processing Index: 177, URL is: https://simple.wikipedia.org/wiki/Brazil\n",
      "Processing Index: 178, URL is: https://simple.wikipedia.org/wiki/Spain\n",
      "Processing Index: 179, URL is: https://simple.wikipedia.org/wiki/Latitude\n",
      "Processing Index: 180, URL is: https://simple.wikipedia.org/wiki/K%C3%B6ppen_climate_classification\n",
      "Processing Index: 181, URL is: https://simple.wikipedia.org/wiki/Evapotranspiration\n",
      "Processing Index: 182, URL is: https://simple.wikipedia.org/wiki/Biodiversity\n",
      "Processing Index: 183, URL is: https://simple.wikipedia.org/wiki/Air_mass\n",
      "Processing Index: 184, URL is: https://simple.wikipedia.org/wiki/Category:Climate\n",
      "Processing Index: 185, URL is: https://simple.wikipedia.org/wiki/Special:WhatLinksHere/Climate\n",
      "Processing Index: 186, URL is: https://simple.wikipedia.org/wiki/Special:RecentChangesLinked/Climate\n",
      "Processing Index: 187, URL is: https://simple.wikipedia.org/wiki/Talk:Climate\n",
      "Processing Index: 188, URL is: https://simple.wikipedia.org/wiki/File:Earth_symbol.svg\n",
      "Processing Index: 189, URL is: https://simple.wikipedia.org/wiki/File:Moon,_Earth_size_comparison_(cropped).jpg\n",
      "Processing Index: 190, URL is: https://simple.wikipedia.org/wiki/The_Blue_Marble\n",
      "Processing Index: 191, URL is: https://simple.wikipedia.org/wiki/Apollo_17\n",
      "Processing Index: 192, URL is: https://simple.wikipedia.org/wiki/Astronaut\n",
      "Processing Index: 193, URL is: https://simple.wikipedia.org/wiki/List_of_adjectivals_and_demonyms_of_astronomical_bodies\n",
      "Processing Index: 194, URL is: https://simple.wikipedia.org/wiki/Osculating_orbit\n",
      "Processing Index: 195, URL is: https://simple.wikipedia.org/wiki/Epoch_(astronomy)\n",
      "Processing Index: 196, URL is: https://simple.wikipedia.org/wiki/J2000.0\n",
      "Processing Index: 197, URL is: https://simple.wikipedia.org/wiki/Perihelion_and_aphelion\n",
      "Processing Index: 198, URL is: https://simple.wikipedia.org/wiki/Semi-major_and_semi-minor_axes\n",
      "Processing Index: 199, URL is: https://simple.wikipedia.org/wiki/Orbital_eccentricity\n",
      "Processing Index: 200, URL is: https://simple.wikipedia.org/wiki/Orbital_period\n",
      "Process completed successfully...\n"
     ]
    }
   ],
   "source": [
    "#checking if main function is executing\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #start crawling from INITIAL url\n",
    "    data_dict = processCrawling(init_url)\n",
    "    \n",
    "    #verifying if crawling was successfull\n",
    "    if data_dict[key_code] == code_success:\n",
    "        \n",
    "        #printing final process response\n",
    "        print(dictToDataframe(data_dict[key_data])[key_data])\n",
    "    else:\n",
    "        \n",
    "        #printing failed crawling response message\n",
    "        print(f'Unable to generate dataframe due to {data_dict[key_data]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
