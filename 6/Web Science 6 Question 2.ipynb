{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb32d07b",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faeecafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dba14b",
   "metadata": {},
   "source": [
    "# Reading Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a21cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = open('Text_0.txt',encoding='utf-8')\n",
    "text_0 = i.read()\n",
    "j = open('Text_1.txt',encoding='utf-8')\n",
    "text_1 = j.read()\n",
    "k = open('Text_2.txt',encoding='utf-8')\n",
    "text_2 = k.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9775c",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df7b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When talking about text similarity  different people have a slightly different notion on what text similarity means In essence  the goal is to compute how  close two pieces of text are in     meaning or     surface closeness The first is referred to as semantic similarity and the latter is referred to as lexical similarity Although the methods for lexical similarity are often used to achieve semantic similarity  to a certain extent   achieving true semantic similarity is often much more involved In this article  I mainly focus on lexical similarity as it has the most use from a practical stand point and then I briefly introduce semantic similarity For the most part  when referring to text similarity  people actually refer to how similar two pieces of text are at the surface level For example  how similar are the phrases  the cat ate the mouse  with  the mouse ate the cat food  by just looking at the words   On the surface  if you consider only word level similarity  these two phrases  with determiners disregarded  appear very similar as   of the   unique words are an exact overlap\n",
      "The formation of sentences is a highly structured and history dependent process The probability of using a specific word in a sentence strongly depends on the  history of word usage earlier in that sentence We study a simple history dependent model of text generation assuming that the sample space of word usage reduces along sentence formation  on average We first show that the model explains the approximate Zipf law found in word frequencies as a direct consequence of sample space reduction We then empirically quantify the amount of sample space reduction in the sentences of    famous English books  by analysis of corresponding word transition tables that capture which words can follow any given word in a text We find a highly nested structure in these transition tables and show that this  nestedness is tightly related to the power law exponents of the observed word frequency distributions With the proposed model  it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent and that deviations from the exact Zipf law can be understood by variations of the degree of nestedness on a book by book basis \n",
      "Zipfs law has been found in many human related fields  including language  where the frequency of a word is persistently found as a power law function of its frequency rank  known as Zipfs law However  there is much dispute whether it is a universal law or a statistical artifact  and little is known about what mechanisms may have shaped it To answer these questions  this study conducted a large scale cross language investigation into Zipfs law The statistical results show that Zipfs laws in    languages all share a   segment structural pattern  with each segment demonstrating distinctive linguistic properties and the lower segment invariably bending downwards to deviate from theoretical expectation This finding indicates that this deviation is a fundamental and universal feature of word frequency distributions in natural languages  not the statistical error of low frequency words A computer simulation based on the dual process theory yields Zipfs law with the same structural pattern  suggesting that Zipfs law of natural languages are motivated by common cognitive mechanisms These results show that Zipfs law in languages is motivated by cognitive mechanisms like dual processing that govern human verbal behaviors In an effort to explore the motivations of Zipfs laws  we conducted a case study on natural languages Though this law was first observed in language  its significance in verbal communication and deep level motivations are far from being clear We know neither why this law has been persistently found in samples of different languages  and with different sizes  nor what underlying mechanisms may have shaped it As a result  it is often proposed that this law has no linguistic significance and cognitive motivations  simple and meaningless statistical processes       such as random typing model             can result in power law distributions similar to Zipfs law as well However  a law usually describes the fundamental nature of something The failure to pin down deep level motivations actually suggests that Zipfs law is not a law  it sheds no light on the fundamental nature of verbal communication and the cognitive mechanisms underlying it To trace the possible cognitive mechanism that shapes this law  it is necessary to find a universally valid linguistic interpretation of Zipfs law  or rather its universal significance in verbal communication  which means that Zipfs curve  word frequency as a function of rank on a log log scale  should exhibit  across different languages  similar structural patterns that have similar linguistic significance Hence  corpus based investigations are needed that cover languages as diverse as possible Nevertheless  previous studies were generally limited to a rather limited range of languages and paid little attention to its underlying cognitive mechanism and significance in language communication \n"
     ]
    }
   ],
   "source": [
    "punc_space = [\"(\",\")\",\"+\",\"≈\",\"=\",'\"',\"-\",\"?\",\"&\",\"%\",':',\"\\n\",\"/\",\",\",\";\",'“', '”','[',']',\"‘\"]\n",
    "punc_no_space = [\"’\",\".\",\"'\"]\n",
    "punc_num = r'[0-9]'\n",
    "p = re.compile('[^a-z0-9]+')\n",
    "pattern = re.compile(\"[\" + re.escape(\"\".join(punc_space)) + \"]\")\n",
    "pattern2 = re.compile(\"[\" + re.escape(\"\".join(punc_no_space)) + \"]\") \n",
    "\n",
    "txt01 = re.sub(pattern, \" \", text_0)\n",
    "txt02 = re.sub(pattern2, \"\", txt01)\n",
    "txt0 = re.sub(punc_num,\" \",txt02)\n",
    "\n",
    "txt11 = re.sub(pattern, \" \", text_1)\n",
    "txt12 = re.sub(pattern2, \"\", txt11)\n",
    "txt1 = re.sub(punc_num,\" \",txt12)\n",
    "\n",
    "txt21 = re.sub(pattern, \" \", text_2)\n",
    "txt22 = re.sub(pattern2, \"\", txt21)\n",
    "txt2 = re.sub(punc_num,\" \",txt22)\n",
    "\n",
    "print(txt0)\n",
    "print(txt1)\n",
    "print(txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdab409",
   "metadata": {},
   "source": [
    "# Jaccard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbad7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11475409836065574\n",
      "0.13962264150943396\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "union01 = set(str(txt0).split()).union(str(txt1).split())\n",
    "intersection01 = set(str(txt0).split()).intersection(str(txt1).split())\n",
    "jaccard01 = len(intersection01)/len(union01)\n",
    "print(jaccard01)\n",
    "\n",
    "union12 = set(str(txt1).split()).union(str(txt2).split())\n",
    "intersection12 = set(str(txt1).split()).intersection(str(txt2).split())\n",
    "jaccard12 = len(intersection12)/len(union12)\n",
    "print(jaccard12)\n",
    "\n",
    "union02 = set(str(txt0).split()).union(str(txt2).split())\n",
    "intersection02 = set(str(txt0).split()).intersection(str(txt2).split())\n",
    "jaccard02 = len(intersection02)/len(union02)\n",
    "print(jaccard02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7d424",
   "metadata": {},
   "source": [
    "# Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1bc4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt0</th>\n",
       "      <th>txt1</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Although</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>For</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          txt0  txt1  distance\n",
       "Although     1     0         1\n",
       "English      0     1         1\n",
       "For          2     0         4\n",
       "I            2     0         4\n",
       "In           2     0         4\n",
       "...        ...   ...       ...\n",
       "which        0     1         1\n",
       "with         2     0         4\n",
       "word         1     7        36\n",
       "words        2     1         1\n",
       "you          1     0         1\n",
       "\n",
       "[183 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = txt0.split()\n",
    "t1 = txt1.split()\n",
    "wordSet = set(t0).union(set(t1))\n",
    "dic0 = dict.fromkeys(wordSet, 0) \n",
    "dic1 = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "for word in t0:\n",
    "    dic0[word]+=1\n",
    "for word in t1:\n",
    "    dic1[word]+=1\n",
    "    \n",
    "df01 = pd.DataFrame([dic0, dic1],index=['txt0','txt1'])\n",
    "df01 = df01.T\n",
    "df01['distance'] = ((df01['txt0']-df01['txt1'])**2)\n",
    "df01.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65ea104",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = txt1.split()\n",
    "t2 = txt2.split()\n",
    "wordSet = set(t1).union(set(t2))\n",
    "dic1 = dict.fromkeys(wordSet, 0) \n",
    "dic2 = dict.fromkeys(wordSet, 0)\n",
    "for word in t1:\n",
    "    dic1[word]+=1\n",
    "for word in t2:\n",
    "    dic2[word]+=1\n",
    "    \n",
    "df12 = pd.DataFrame([dic1, dic2],index=['txt1','txt2'])\n",
    "df12 = df12.T\n",
    "df12['distance'] = ((df12['txt1']-df12['txt2'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79498731",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = txt0.split()\n",
    "t2 = txt2.split()\n",
    "wordSet = set(t0).union(set(t2))\n",
    "dic0 = dict.fromkeys(wordSet, 0) \n",
    "dic2 = dict.fromkeys(wordSet, 0) \n",
    "for word in t0:\n",
    "    dic0[word]+=1\n",
    "for word in t2:\n",
    "    dic2[word]+=1\n",
    "    \n",
    "df02 = pd.DataFrame([dic0, dic2],index=['txt0','txt2'])\n",
    "df02 = df02.T\n",
    "df02['distance'] = ((df02['txt0']-df02['txt2'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c4d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.930952282978865\n",
      "38.58756276314948\n",
      "44.78839135311738\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance\n",
    "ed01 = (df01['distance'].sum())**0.5\n",
    "ed12 = (df12['distance'].sum())**0.5\n",
    "ed02 = (df02['distance'].sum())**0.5\n",
    "print(ed01)\n",
    "print(ed12)\n",
    "print(ed02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef6eb0",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717c533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01['txt0*txt1'] = df01['txt0']*df01['txt1']\n",
    "df01['sq(tx0)'] =  df01['txt0']**2\n",
    "df01['sq(tx1)'] =  df01['txt1']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42b579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['txt1*txt2'] = df12['txt1']*df12['txt2']\n",
    "df12['sq(tx1)'] =  df12['txt1']**2\n",
    "df12['sq(tx2)'] =  df12['txt2']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3993ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df02['txt0*txt2'] = df02['txt0']*df02['txt2']\n",
    "df02['sq(tx0)'] =  df02['txt0']**2\n",
    "df02['sq(tx2)'] =  df02['txt2']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbcf3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0948163852542165\n",
      "0.9495990273880835\n",
      "1.1900355260968518\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity\n",
    "cs01 = (df01['txt0*txt1'].sum())/(((df01['sq(tx0)'].sum())**0.5) * ((df01['sq(tx1)'].sum())**0.5))\n",
    "cs12 = (df12['txt1*txt2'].sum())/(((df12['sq(tx1)'].sum())**0.5) * ((df12['sq(tx2)'].sum())**0.5))\n",
    "cs02 = (df02['txt0*txt2'].sum())/(((df02['sq(tx0)'].sum())**0.5) * ((df02['sq(tx2)'].sum())**0.5))\n",
    "cs01 = math.acos(cs01)\n",
    "cs12 = math.acos(cs12)\n",
    "cs02 = math.acos(cs02)\n",
    "print(cs01)\n",
    "print(cs12)\n",
    "print(cs02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae472934",
   "metadata": {},
   "source": [
    "# Interpretation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e2a2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = [jaccard01,jaccard12,jaccard02]\n",
    "ed = [ed01,ed12,ed02]\n",
    "cs = [cs01,cs12,cs02]\n",
    "df = pd.DataFrame(data = [jaccard,ed,cs],index=['Jaccard Score','Euclidean Distance','Cosine Similarity'],columns = ['d(0,1)','d(1,2)','d(0,2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b3453b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d(0,1)</th>\n",
       "      <th>d(1,2)</th>\n",
       "      <th>d(0,2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jaccard Score</th>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.139623</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euclidean Distance</th>\n",
       "      <td>28.930952</td>\n",
       "      <td>38.587563</td>\n",
       "      <td>44.788391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <td>1.094816</td>\n",
       "      <td>0.949599</td>\n",
       "      <td>1.190036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       d(0,1)     d(1,2)     d(0,2)\n",
       "Jaccard Score        0.114754   0.139623   0.125000\n",
       "Euclidean Distance  28.930952  38.587563  44.788391\n",
       "Cosine Similarity    1.094816   0.949599   1.190036"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Different choice of metric is interpreting different results.\"\n",
    "\"For Jaccard Score and Euclidean Distance, text file text_0 and text_1 has the smallest distance between them.\"\n",
    "\"But for Cosine Similarity, distance between text files text_1 and text_2 has smallest distance between them.\"\n",
    "\"Choice of metric is basically part of the madel but usually Cosine Similarity is considered.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
